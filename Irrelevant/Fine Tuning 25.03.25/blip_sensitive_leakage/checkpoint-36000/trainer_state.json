{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 36000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.027777777777777776,
      "grad_norm": 16.03749656677246,
      "learning_rate": 4.9537037037037035e-05,
      "loss": 7.5663,
      "step": 500
    },
    {
      "epoch": 0.05555555555555555,
      "grad_norm": 12.228013038635254,
      "learning_rate": 4.9074074074074075e-05,
      "loss": 6.1366,
      "step": 1000
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 7.730826377868652,
      "learning_rate": 4.8611111111111115e-05,
      "loss": 5.8033,
      "step": 1500
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 12.567976951599121,
      "learning_rate": 4.814814814814815e-05,
      "loss": 5.5769,
      "step": 2000
    },
    {
      "epoch": 0.1388888888888889,
      "grad_norm": 25.57480812072754,
      "learning_rate": 4.768518518518519e-05,
      "loss": 5.4658,
      "step": 2500
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 19.31586265563965,
      "learning_rate": 4.722222222222222e-05,
      "loss": 5.299,
      "step": 3000
    },
    {
      "epoch": 0.19444444444444445,
      "grad_norm": 13.603413581848145,
      "learning_rate": 4.675925925925926e-05,
      "loss": 5.2815,
      "step": 3500
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 12.440918922424316,
      "learning_rate": 4.62962962962963e-05,
      "loss": 5.1076,
      "step": 4000
    },
    {
      "epoch": 0.25,
      "grad_norm": 9.422795295715332,
      "learning_rate": 4.5833333333333334e-05,
      "loss": 4.9948,
      "step": 4500
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 9.225577354431152,
      "learning_rate": 4.5370370370370374e-05,
      "loss": 4.9849,
      "step": 5000
    },
    {
      "epoch": 0.3055555555555556,
      "grad_norm": 9.699400901794434,
      "learning_rate": 4.490740740740741e-05,
      "loss": 4.8503,
      "step": 5500
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 20.543916702270508,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 4.8745,
      "step": 6000
    },
    {
      "epoch": 0.3611111111111111,
      "grad_norm": 11.958643913269043,
      "learning_rate": 4.3981481481481486e-05,
      "loss": 4.8441,
      "step": 6500
    },
    {
      "epoch": 0.3888888888888889,
      "grad_norm": 14.16321086883545,
      "learning_rate": 4.351851851851852e-05,
      "loss": 4.8211,
      "step": 7000
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 12.811197280883789,
      "learning_rate": 4.305555555555556e-05,
      "loss": 4.8,
      "step": 7500
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 14.612160682678223,
      "learning_rate": 4.259259259259259e-05,
      "loss": 4.6932,
      "step": 8000
    },
    {
      "epoch": 0.4722222222222222,
      "grad_norm": 11.388416290283203,
      "learning_rate": 4.212962962962963e-05,
      "loss": 4.6973,
      "step": 8500
    },
    {
      "epoch": 0.5,
      "grad_norm": 23.748441696166992,
      "learning_rate": 4.166666666666667e-05,
      "loss": 4.6392,
      "step": 9000
    },
    {
      "epoch": 0.5277777777777778,
      "grad_norm": 17.22140884399414,
      "learning_rate": 4.1203703703703705e-05,
      "loss": 4.5799,
      "step": 9500
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 29.04564094543457,
      "learning_rate": 4.074074074074074e-05,
      "loss": 4.6368,
      "step": 10000
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 9.488640785217285,
      "learning_rate": 4.027777777777778e-05,
      "loss": 4.6205,
      "step": 10500
    },
    {
      "epoch": 0.6111111111111112,
      "grad_norm": 9.147833824157715,
      "learning_rate": 3.981481481481482e-05,
      "loss": 4.6187,
      "step": 11000
    },
    {
      "epoch": 0.6388888888888888,
      "grad_norm": 16.055084228515625,
      "learning_rate": 3.935185185185186e-05,
      "loss": 4.523,
      "step": 11500
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 11.017436027526855,
      "learning_rate": 3.888888888888889e-05,
      "loss": 4.547,
      "step": 12000
    },
    {
      "epoch": 0.6944444444444444,
      "grad_norm": 7.324373245239258,
      "learning_rate": 3.8425925925925924e-05,
      "loss": 4.4568,
      "step": 12500
    },
    {
      "epoch": 0.7222222222222222,
      "grad_norm": 13.946392059326172,
      "learning_rate": 3.7962962962962964e-05,
      "loss": 4.5882,
      "step": 13000
    },
    {
      "epoch": 0.75,
      "grad_norm": 10.274140357971191,
      "learning_rate": 3.7500000000000003e-05,
      "loss": 4.5059,
      "step": 13500
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 10.582110404968262,
      "learning_rate": 3.7037037037037037e-05,
      "loss": 4.4304,
      "step": 14000
    },
    {
      "epoch": 0.8055555555555556,
      "grad_norm": 9.98686695098877,
      "learning_rate": 3.6574074074074076e-05,
      "loss": 4.552,
      "step": 14500
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 12.120319366455078,
      "learning_rate": 3.611111111111111e-05,
      "loss": 4.4791,
      "step": 15000
    },
    {
      "epoch": 0.8611111111111112,
      "grad_norm": 8.349813461303711,
      "learning_rate": 3.564814814814815e-05,
      "loss": 4.4897,
      "step": 15500
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 14.378971099853516,
      "learning_rate": 3.518518518518519e-05,
      "loss": 4.5127,
      "step": 16000
    },
    {
      "epoch": 0.9166666666666666,
      "grad_norm": 9.721735000610352,
      "learning_rate": 3.472222222222222e-05,
      "loss": 4.4716,
      "step": 16500
    },
    {
      "epoch": 0.9444444444444444,
      "grad_norm": 17.076339721679688,
      "learning_rate": 3.425925925925926e-05,
      "loss": 4.418,
      "step": 17000
    },
    {
      "epoch": 0.9722222222222222,
      "grad_norm": 10.242986679077148,
      "learning_rate": 3.3796296296296295e-05,
      "loss": 4.4328,
      "step": 17500
    },
    {
      "epoch": 1.0,
      "grad_norm": 12.584537506103516,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 4.3881,
      "step": 18000
    },
    {
      "epoch": 1.0,
      "eval_runtime": 3284.2991,
      "eval_samples_per_second": 2.436,
      "eval_steps_per_second": 0.609,
      "step": 18000
    },
    {
      "epoch": 1.0277777777777777,
      "grad_norm": 12.397329330444336,
      "learning_rate": 3.2870370370370375e-05,
      "loss": 4.4037,
      "step": 18500
    },
    {
      "epoch": 1.0555555555555556,
      "grad_norm": 9.020339965820312,
      "learning_rate": 3.240740740740741e-05,
      "loss": 4.3562,
      "step": 19000
    },
    {
      "epoch": 1.0833333333333333,
      "grad_norm": 10.112750053405762,
      "learning_rate": 3.194444444444444e-05,
      "loss": 4.6108,
      "step": 19500
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 11.385772705078125,
      "learning_rate": 3.148148148148148e-05,
      "loss": 4.4516,
      "step": 20000
    },
    {
      "epoch": 1.1388888888888888,
      "grad_norm": 7.742003440856934,
      "learning_rate": 3.101851851851852e-05,
      "loss": 4.4235,
      "step": 20500
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 13.922592163085938,
      "learning_rate": 3.055555555555556e-05,
      "loss": 4.3822,
      "step": 21000
    },
    {
      "epoch": 1.1944444444444444,
      "grad_norm": 10.301591873168945,
      "learning_rate": 3.0092592592592593e-05,
      "loss": 4.34,
      "step": 21500
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 6.375320911407471,
      "learning_rate": 2.962962962962963e-05,
      "loss": 4.2825,
      "step": 22000
    },
    {
      "epoch": 1.25,
      "grad_norm": 10.672985076904297,
      "learning_rate": 2.916666666666667e-05,
      "loss": 4.3833,
      "step": 22500
    },
    {
      "epoch": 1.2777777777777777,
      "grad_norm": 8.116583824157715,
      "learning_rate": 2.8703703703703706e-05,
      "loss": 4.4386,
      "step": 23000
    },
    {
      "epoch": 1.3055555555555556,
      "grad_norm": 18.996307373046875,
      "learning_rate": 2.824074074074074e-05,
      "loss": 4.4267,
      "step": 23500
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 10.063196182250977,
      "learning_rate": 2.777777777777778e-05,
      "loss": 4.4106,
      "step": 24000
    },
    {
      "epoch": 1.3611111111111112,
      "grad_norm": 12.29175090789795,
      "learning_rate": 2.7314814814814816e-05,
      "loss": 4.3843,
      "step": 24500
    },
    {
      "epoch": 1.3888888888888888,
      "grad_norm": 11.164688110351562,
      "learning_rate": 2.6851851851851855e-05,
      "loss": 4.3171,
      "step": 25000
    },
    {
      "epoch": 1.4166666666666667,
      "grad_norm": 13.111364364624023,
      "learning_rate": 2.6388888888888892e-05,
      "loss": 4.2675,
      "step": 25500
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 9.032883644104004,
      "learning_rate": 2.5925925925925925e-05,
      "loss": 4.2799,
      "step": 26000
    },
    {
      "epoch": 1.4722222222222223,
      "grad_norm": 8.081902503967285,
      "learning_rate": 2.5462962962962965e-05,
      "loss": 4.286,
      "step": 26500
    },
    {
      "epoch": 1.5,
      "grad_norm": 14.083076477050781,
      "learning_rate": 2.5e-05,
      "loss": 4.3658,
      "step": 27000
    },
    {
      "epoch": 1.5277777777777777,
      "grad_norm": 24.93996238708496,
      "learning_rate": 2.4537037037037038e-05,
      "loss": 4.4056,
      "step": 27500
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 6.708826541900635,
      "learning_rate": 2.4074074074074074e-05,
      "loss": 4.3119,
      "step": 28000
    },
    {
      "epoch": 1.5833333333333335,
      "grad_norm": 12.681124687194824,
      "learning_rate": 2.361111111111111e-05,
      "loss": 4.3289,
      "step": 28500
    },
    {
      "epoch": 1.6111111111111112,
      "grad_norm": 5.591711044311523,
      "learning_rate": 2.314814814814815e-05,
      "loss": 4.2721,
      "step": 29000
    },
    {
      "epoch": 1.6388888888888888,
      "grad_norm": 9.379217147827148,
      "learning_rate": 2.2685185185185187e-05,
      "loss": 4.3253,
      "step": 29500
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 2.903620958328247,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 4.352,
      "step": 30000
    },
    {
      "epoch": 1.6944444444444444,
      "grad_norm": 12.658773422241211,
      "learning_rate": 2.175925925925926e-05,
      "loss": 4.2551,
      "step": 30500
    },
    {
      "epoch": 1.7222222222222223,
      "grad_norm": 7.005273818969727,
      "learning_rate": 2.1296296296296296e-05,
      "loss": 4.2273,
      "step": 31000
    },
    {
      "epoch": 1.75,
      "grad_norm": 10.511946678161621,
      "learning_rate": 2.0833333333333336e-05,
      "loss": 4.3409,
      "step": 31500
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 8.909723281860352,
      "learning_rate": 2.037037037037037e-05,
      "loss": 4.3583,
      "step": 32000
    },
    {
      "epoch": 1.8055555555555556,
      "grad_norm": 12.303546905517578,
      "learning_rate": 1.990740740740741e-05,
      "loss": 4.2401,
      "step": 32500
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 11.979331016540527,
      "learning_rate": 1.9444444444444445e-05,
      "loss": 4.2995,
      "step": 33000
    },
    {
      "epoch": 1.8611111111111112,
      "grad_norm": 12.605134963989258,
      "learning_rate": 1.8981481481481482e-05,
      "loss": 4.2727,
      "step": 33500
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 14.335479736328125,
      "learning_rate": 1.8518518518518518e-05,
      "loss": 4.2465,
      "step": 34000
    },
    {
      "epoch": 1.9166666666666665,
      "grad_norm": 10.01660442352295,
      "learning_rate": 1.8055555555555555e-05,
      "loss": 4.2219,
      "step": 34500
    },
    {
      "epoch": 1.9444444444444444,
      "grad_norm": 19.09801483154297,
      "learning_rate": 1.7592592592592595e-05,
      "loss": 4.2096,
      "step": 35000
    },
    {
      "epoch": 1.9722222222222223,
      "grad_norm": 10.302543640136719,
      "learning_rate": 1.712962962962963e-05,
      "loss": 4.2778,
      "step": 35500
    },
    {
      "epoch": 2.0,
      "grad_norm": 7.581793308258057,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 4.372,
      "step": 36000
    },
    {
      "epoch": 2.0,
      "eval_runtime": 3294.5099,
      "eval_samples_per_second": 2.428,
      "eval_steps_per_second": 0.607,
      "step": 36000
    }
  ],
  "logging_steps": 500,
  "max_steps": 54000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8.573479872469402e+19,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
